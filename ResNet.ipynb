{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMJ0NvfLu+jXKfrRq/MLy6H"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"GpPrSv-cEp4u"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zrj6KxaMFIMr"},"source":["### Degradation問題\n","Degradation問題とは層が深くなれば深くなるほど表現力が上がってくるが、学習フェーズでなぜか浅い層に負ける。というように層を深くしたとしても必ずしも訓練誤差が下がっていくというわけではない。<br>\n","Microsoft社は勾配も消失していないと確認はしたけれども56層でできたニューラルネットワークは20層でできたニューラルネットワークには勝てないといった問題があった。"]},{"cell_type":"markdown","metadata":{"id":"92ZTyJctKBhP"},"source":["### 勾配消失が起きる理由\n","　DLで学習する際には各層ごとに活性化関数を微分して勾配を求める。まず入が層に近い『浅い』層ではその層への入力と出力の差(accuracy)が大きいため、微分操作は有効に働く。しかし、学習が層の『深い』段階へと進むにつれ、（当たり前だが）学習が収束に近づくと入力から出力への変換精度がどんどん上がっていくため、入力と出力の差は極めて小さくなり、勾配を取りにくくなる。層から層への伝播は掛け算の性質をもっているため、この差の減少傾向は指数関数的になる。つまり、層を増やしていくと、あっという間に勾配が消えてなくなってしまうのである。(http://terada-h.hatenablog.com/entry/2016/12/13/192940#:~:text=Residual%20Learning%EF%BC%88%E6%AE%8B%E5%B7%AE%E5%AD%A6%E7%BF%92,%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E5%BD%A2%E3%81%AB%E3%81%99%E3%82%8B%E3%80%82)<br>\n","要は誤差逆伝播法でどんどんチェーンルールによって入力層に向かって更新していくのだが、深くしすぎることによってだんだんと反映されなくなってしまう。"]},{"cell_type":"code","metadata":{"id":"BNQ3aqV7F6VV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZ0_9AbfF7cI"},"source":["### では深い層をどのようにすれば浅いモデルと同じような制度を生み出すことが出来るのか？？\n","\n","→深い層を恒等関数にしてしまえば少なくとも浅いモデルと同じ精度になる。(udemy pytorch講座の28.ResNet概要5分ぐらいのところを見るとよい！)残差を学習していくという感じ<br>\n","入力をそのままスキップさせてしまうようにすれば実現可能（スキップコネクション）<br>\n","またミニバッチの中で正規化を行っていくと過学習しにくく学習が安定していく。（BatchNormalization）（バッチごとに平均と分散を計算していき、正規化を行っていく。udemyの\n","28．7分のところを確認）<br>\n","BatchNormalizationに関して(https://deepage.net/deep_learning/2016/10/26/batch_normalization.html)基本的には、勾配消失・爆発を防ぐための手法\n"]},{"cell_type":"code","metadata":{"id":"YmGodvL6Hq_P","executionInfo":{"status":"ok","timestamp":1602917659669,"user_tz":-540,"elapsed":3592,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}}},"source":["import torch \n","import torch.nn as nn"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xaJp7HmVkMF","executionInfo":{"status":"ok","timestamp":1602919795310,"user_tz":-540,"elapsed":923,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}}},"source":["class ResidualBlock(nn.Module):\n","    #変数定義のところで変数を定義し\n","    #ニューラルネットはdef forwardのところで定義していく。\n","    def __init__(self , in_channels , out_channels):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels , out_channels , kernel_size = 3 , padding=1)\n","        self.conv2 = nn.Conv2d(out_channels , out_channels , kernel_size=3 , padding = 1)\n","        self.conv3 = nn.Conv2d(in_channels , out_channels , kernel_size=1 , padding=0)#あるチャネル数を持っているものを別のチャネル数に変更するというもの。\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    #出力されたものに対して入力値を足す際にチャネル数が違うのでチャネル数を合わせるための関数を定義している。\n","    def shortcut(self , x):\n","        x = self.conv3(x)#conv3でチャネル数がin_channelからout_channelに変更を行うことが出来る。\n","        x = self.bn(x)\n","        return x\n","\n","    #残差ブロックの作成を行っている。\n","    def forward(self , x):\n","        identity = x #そのまま入力した値を代入している。\n","        x = self.conv1(x)\n","        x = self.bn(x)#Batch Normalizationを行っている。これを行うことによって学習の収束速度が上昇し、正則化の効果があったりしていいことが多かったりする\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn(x)\n","        x += self.shortcut(identity) #出力されたものに対して入力したものを足している。\n","        return x\n","    "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KlAl--UadwZu","executionInfo":{"status":"ok","timestamp":1602921336998,"user_tz":-540,"elapsed":986,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}}},"source":["class ResNet(nn.Module):\n","    def __init__(self , block) :#blockには先ほど定義した ResidualBlockを渡す。\n","        super().__init__()\n","        self.linear = nn.Linear(28*28*64 , out_features=10)#28*28の画像が64チャネルが入力される。10クラスの分類を行うと仮定して行っている。\n","        # _make_layerは下で定義している。\n","        self.layer = self._make_layer(block , 3 ,3, 64)#ブロックを3つ積み上げ、カラー画像を入力するとしてin_channelsは3,out_channelsは任意で指定し今回は64\n","\n","    #ブロックをどんどん積み上げていく関数\n","    def _make_layer(self , block , num_residual_blocks , in_channels , out_channels ):#num_residual_blocksにはresidual_blockを何層作成するのかを入力する。\n","        layers = []\n","        for i in range(num_residual_blocks):\n","            if i == 0:#1層目に関してはチャネル数が画像のチャネル数なので分岐する必要がある\n","                layers.append(block(in_channels , out_channels))\n","            else:\n","                layers.append(block(out_channels , out_channels))\n","        return nn.Sequential(*layers)#複数のブロックを積み重ねたそうを出力している。\n","\n","    def forward(self , x):\n","        x = self.layer(x)\n","        x = x.view(x.size(0) , -1)#Linearの層にデータを入れるため、データを一次元に変換している。\n","        x = self.linear(x)\n","        return x\n","\n","\n","\n","            \n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"vyUYByoIjVeK","executionInfo":{"status":"ok","timestamp":1602921337458,"user_tz":-540,"elapsed":987,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}}},"source":["#モデルをインスタンス化\n","model = ResNet(ResidualBlock)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fKUhaE_jfkA","executionInfo":{"status":"ok","timestamp":1602921346432,"user_tz":-540,"elapsed":1070,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}},"outputId":"f0bff723-2653-4781-ed75-62048762e43d","colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["model"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (linear): Linear(in_features=50176, out_features=10, bias=True)\n","  (layer): Sequential(\n","    (0): ResidualBlock(\n","      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (conv3): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (1): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","    (2): ResidualBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"gXIbq2O4jqzy","executionInfo":{"status":"ok","timestamp":1602921440744,"user_tz":-540,"elapsed":970,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}}},"source":["x_test = torch.randn(32 , 3 , 28 , 28)#バッチ数、チャネル数、縦＊横の大きさ"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRXTgN0mkCD2","executionInfo":{"status":"ok","timestamp":1602921456228,"user_tz":-540,"elapsed":1323,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}}},"source":["out_put = model(x_test)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7IDalxekFz_","executionInfo":{"status":"ok","timestamp":1602921468951,"user_tz":-540,"elapsed":1092,"user":{"displayName":"kiichiro inami","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSFTfvEy3g3dRXxNRymOSPYJuuKdMicvgGU_Tf=s64","userId":"11382466733832066009"}},"outputId":"e3110d25-a679-4f44-c273-bbdb68aca020","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["out_put.size()#ミニバッチサイズで10クラスになっていることが確認できる。"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 10])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Yd9BTzNUkIxM"},"source":[""],"execution_count":null,"outputs":[]}]}